{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaraMagdy2/Flutter_Listview/blob/master/Clara_Magdy_Ghaly_NLP_FGP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cf6b243",
      "metadata": {
        "id": "8cf6b243"
      },
      "source": [
        "# Toxic Comments NLP By Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b5c1b8",
      "metadata": {
        "id": "74b5c1b8"
      },
      "source": [
        "## build transformer from scratch according to Attention is all we Need paper using pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e281ff",
      "metadata": {
        "id": "d7e281ff"
      },
      "source": [
        "![2023-09-14%20%281%29.png](attachment:2023-09-14%20%281%29.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be38bc60",
      "metadata": {
        "id": "be38bc60"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea65de3",
      "metadata": {
        "id": "bea65de3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn#provides various classes and functions for defining and training neural network models.\n",
        "import math#provides mathematical functions and constants."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16fefcef",
      "metadata": {
        "id": "16fefcef"
      },
      "source": [
        " input embeddings play a crucial role in many NLP and machine learning applications by providing a way to represent and process textual or sequential data in a format that neural networks can effectively learn from. They capture the semantic relationships between tokens and enable models to understand and generate natural language text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871554ee",
      "metadata": {
        "id": "871554ee"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, input_vocab_size, d_model):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape (batch_size, seq_length)\n",
        "        # Apply the embedding layer\n",
        "        embedded_x = self.embedding(x)\n",
        "        return embedded_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6098c8",
      "metadata": {
        "id": "eb6098c8"
      },
      "source": [
        "This InputEmbedding class is typically used as a component within a larger neural network, especially in natural language processing tasks. It's responsible for the initial transformation of input tokens into continuous vector representations, making them suitable for further processing by other layers of the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f12b419",
      "metadata": {
        "id": "7f12b419"
      },
      "source": [
        "## Build positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cefcb04",
      "metadata": {
        "id": "4cefcb04"
      },
      "source": [
        "it imparts information about word positions, enabling the model to understand and leverage the sequential nature of text data. This is especially important for tasks where word order plays a significant role in determining the meaning or sentiment of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d55f01",
      "metadata": {
        "id": "41d55f01"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):#nn.Module, indicating that it's a PyTorch neural network module.\n",
        "    def __init__(self, d_model, max_seq_length):#d_model,It determines the size of the positional encodings.\n",
        "        #max_seq_length:which is the maximum number of tokens in your input sequence.\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "#This sets up a dropout layer with a dropout probability of 10% (0.1). Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
        "        pe = torch.zeros(max_seq_length, d_model)#- This creates a tensor pe filled with zeros. Its shape is (max_seq_length, d_model).\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "#This generates a tensor position containing a sequence of numbers from 0 to max_seq_length - 1\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#This calculates the values of a term used in the positional encoding formula.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "# This computes the sine component of the positional encoding and assigns it to every second column of the pe tensor.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#This computes the cosine component of the positional encoding and assigns it to the remaining columns of the pe tensor.\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#This reshapes the pe tensor to have a shape of (max_seq_length, 1, d_model) and then transposes it to have a final shape of (1, max_seq_length, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "#buffer:This is necessary because the positional encoding doesn't contain any learnable parameters but should be included in the model's state.\n",
        "    def forward(self, x):\n",
        "# It takes an input tensor x.\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "#This line adds the positional encoding tensor self.pe to the input tensor x. The [:x.size(0), :]\n",
        "        return self.dropout(x)# the output tensor x is passed through the dropout layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1171fca3",
      "metadata": {
        "id": "1171fca3"
      },
      "source": [
        "this code defines a PyTorch module for positional encoding, which is essential for transformer-based models to incorporate information about the positions of words in the input sequence. It calculates positional embeddings based on a mathematical formula involving sine and cosine functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660df51e",
      "metadata": {
        "id": "660df51e"
      },
      "source": [
        "![WhatsApp%20Image%202023-09-14%20at%202.16.53%20PM.jpeg](attachment:WhatsApp%20Image%202023-09-14%20at%202.16.53%20PM.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b863567",
      "metadata": {
        "id": "7b863567"
      },
      "source": [
        "## build Multi head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b3bf6f",
      "metadata": {
        "id": "99b3bf6f"
      },
      "source": [
        "multi-Head Attention in text classification allows the model to capture context and relationships between words in the input text, making it highly effective for understanding and classifying text data, even when dealing with long sequences and complex dependencies between words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486e2a3c",
      "metadata": {
        "id": "486e2a3c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):#num_heads: This is the number of attention heads.\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "#This assertion checks that d_model is divisible by num_heads to ensure that the dimensions align correctly for the multi-head attention mechanism.\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "# This calculates the dimension of each head by dividing d_model by num_heads.\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "#These lines define linear transformation layers using nn.Linear.\n",
        "#These layers are used to project the input queries, keys, and values into the multi-head attention space.\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.d_head)\n",
        "        return x.permute(0, 2, 1, 3)  # Reshape for multi-head attention\n",
        "#This method is used to split the input tensor x into multiple heads.\n",
        "#It reshapes the input tensor to have shape (batch_size, seq_len, num_heads, d_head).\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "#These lines project the query, key, and value tensors into the multi-head attention space by applying the linear transformation layers and then splitting them into multiple heads.\n",
        "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.d_head**0.5)\n",
        "#This calculates the attention scores using matrix multiplication between the query and key tensors\n",
        "        if mask is not None:\n",
        "            scores += mask.unsqueeze(1)\n",
        "#if a mask is provided, it is added to the attention scores to apply masking.\n",
        "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "# This computes the softmax function along the last dimension of the attention scores to obtain attention weights.\n",
        "        weighted_sum = torch.matmul(attention_weights, V)\n",
        "#This computes the weighted sum of the values according to the attention weights.\n",
        "        concat_heads = weighted_sum.permute(0, 2, 1, 3).contiguous().view(query.size(0), -1, self.num_heads * self.d_head)\n",
        "#reshapes and concatenates the output from multiple heads.\n",
        "        output = self.W_o(concat_heads)\n",
        "        return output\n",
        "#the concatenated result is projected back into the original space using the linear transformation layer W_o"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4c58f8",
      "metadata": {
        "id": "3e4c58f8"
      },
      "source": [
        "this code defines a Multi-Head Attention module, which is a fundamental component of transformer-based models. It takes input queries, keys, and values, calculates attention scores, applies attention masking if necessary, computes attention weights, and produces the final output by concatenating and projecting the results from multiple attention heads.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b872d487",
      "metadata": {
        "id": "b872d487"
      },
      "source": [
        "![WhatsApp%20Image%202023-09-14%20at%209.06.20%20PM.jpeg](attachment:WhatsApp%20Image%202023-09-14%20at%209.06.20%20PM.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb104020",
      "metadata": {
        "id": "cb104020"
      },
      "source": [
        "# Build Position wise FeedForward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8367b9ee",
      "metadata": {
        "id": "8367b9ee"
      },
      "source": [
        "This PFF layer can be used in a text classification model as part of the encoder block, following the Multi-Head Attention layer. It allows the model to capture complex patterns and relationships in the data before making classification predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a462f0",
      "metadata": {
        "id": "b4a462f0"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "#d_ff: This parameter represents the dimensionality of the intermediate layer in the feedforward neural network.\n",
        "#This layer is responsible for introducing non-linearity to the model through the subsequent ReLU activation.\n",
        "        self.relu = nn.ReLU()\n",
        "#This line defines the rectified linear unit (ReLU) activation function,which introduces non-linearity by applying the element-wise ReLU function to its input.\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "#This line defines the second linear layer in the feedforward network.\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "#  In the forward pass, the input tensor x is first passed through the first linear layer (self.linear1).\n",
        "# Then, the ReLU activation function is applied (self.relu).\n",
        "# Finally, the result is passed through the second linear layer (self.linear2). The output of the second linear layer is the final output of the feedforward network.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d88c8d9",
      "metadata": {
        "id": "5d88c8d9"
      },
      "source": [
        "PositionwiseFeedForward module is responsible for transforming the input tensor at each position in the sequence through a feedforward neural network. This transformation allows the model to capture complex relationships and patterns in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87febc6",
      "metadata": {
        "id": "c87febc6"
      },
      "source": [
        "![WhatsApp%20Image%202023-09-14%20at%206.40.10%20PM.jpeg](attachment:WhatsApp%20Image%202023-09-14%20at%206.40.10%20PM.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2062203",
      "metadata": {
        "id": "e2062203"
      },
      "source": [
        "## Build layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74ae68d",
      "metadata": {
        "id": "a74ae68d"
      },
      "source": [
        "The key idea is to incorporate LayerNorm within the model architecture to improve training stability and classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166e1ec5",
      "metadata": {
        "id": "166e1ec5"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, epsilon=1e-6):\n",
        "#features: This parameter specifies the number of features (dimensions) that will be normalized independently.\n",
        "#In other words, it defines the size of the last dimension of the input tensor.\n",
        "#epsilon:improve numerical stability during normalization.\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "#This line defines a learnable parameter gamma (scaling factor) as a PyTorch parameter tensor.\n",
        "#It's initialized with ones and has the same shape as the specified number of features.\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "# This line defines another learnable parameter beta (bias) as a PyTorch parameter tensor.\n",
        "#It's initialized with zeros and has the same shape as the specified number of features.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)#This computes the mean independently for each feature.\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
        "#The normalization is done by subtracting the mean and dividing by the standard deviation (plus epsilon for stability)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23769356",
      "metadata": {
        "id": "23769356"
      },
      "source": [
        "the LayerNorm module is a normalization technique applied to neural network activations. It scales and shifts the activations to make them have a mean of zero and a standard deviation of one, independently for each feature. This helps improve the training and generalization of neural networks by reducing the impact of feature scale variations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac7394e",
      "metadata": {
        "id": "4ac7394e"
      },
      "source": [
        "![WhatsApp%20Image%202023-09-14%20at%204.04.36%20PM.jpeg](attachment:WhatsApp%20Image%202023-09-14%20at%204.04.36%20PM.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620f8179",
      "metadata": {
        "id": "620f8179"
      },
      "source": [
        "### I apply residual connections in encoder and decoder layer class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e9a874",
      "metadata": {
        "id": "f1e9a874"
      },
      "source": [
        "residual connections are a fundamental architectural innovation that has had a significant impact on the training of deep neural networks. They address issues related to gradient flow, optimization, and the ability to learn complex mappings, making it possible to train deeper and more powerful models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cff1b8f",
      "metadata": {
        "id": "0cff1b8f"
      },
      "source": [
        "## Build Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1478d3",
      "metadata": {
        "id": "4f1478d3"
      },
      "source": [
        " The key idea is to use a transformer-based encoder to capture contextual information and patterns in the text data, which can then be used for accurate classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2d1a67",
      "metadata": {
        "id": "0f2d1a67"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Position-wise Feed-Forward Network\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "        self.norm2 = LayerNorm(d_model)#This initializes a Layer Normalization layer for the output of the Position-wise Feed-Forward Network.\n",
        "        self.dropout2 = nn.Dropout(p=dropout)#his initializes a dropout layer to be applied after the Position-wise Feed-Forward Network.\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-Head Self-Attention\n",
        "        attention_output = self.self_attention(x, x, x, mask)\n",
        "# This line applies the Multi-Head Attention mechanism to the input tensor x.\n",
        "#It computes attention scores using x as both the query, key, and value.\n",
        "        x = x + self.dropout1(attention_output)\n",
        "#This adds the result of attention to the original input x after applying dropout.\n",
        "        x = self.norm1(x)\n",
        "#Layer normalization is applied to the output of the attention mechanism.\n",
        "        # Position-wise Feed-Forward Network\n",
        "        ffn_output = self.feed_forward(x)# This line applies the Position-wise Feed-Forward Network to the output of the attention layer.\n",
        "        x = x + self.dropout2(ffn_output)\n",
        "#This adds the result of the feed-forward network to the previous output x after applying dropout\n",
        "        x = self.norm2(x)\n",
        "#: Layer normalization is applied to the output of the feed-forward network.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9c3f6c",
      "metadata": {
        "id": "aa9c3f6c"
      },
      "source": [
        "he TransformerEncoderLayer represents a single layer in the transformer encoder. It consists of Multi-Head Self-Attention and a Position-wise Feed-Forward Network, both of which contribute to capturing contextual information and patterns in the data. Layer normalization and dropout are applied to stabilize and regularize the output of these components, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b2f105",
      "metadata": {
        "id": "f4b2f105"
      },
      "source": [
        "## build Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d819b464",
      "metadata": {
        "id": "d819b464"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "#This layer allows the model to attend to different parts of the input sequence simultaneously.\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "# It performs layer normalization on the output of the self-attention layer.\n",
        "        # Position-wise Feed-Forward Layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "# #which is applied to the output of the layer normalization.\n",
        "# It consists of two linear transformations with a ReLU activation in between.\n",
        "# The first linear layer reduces the dimensionality of the input,\n",
        "# and the second linear layer brings it back to the original dimension.\n",
        "        # Layer Normalization\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "#is another layer normalization applied after the position-wise feed-forward network.\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, self_mask=None, enc_mask=None):\n",
        "        # Multi-Head Self-Attention\n",
        "        attention_output, _ = self.self_attention(x, x, x, attn_mask=self_mask)\n",
        "\n",
        "        # Residual Connection and Layer Normalization\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "\n",
        "        # Position-wise Feed-Forward Network\n",
        "        ff_output = self.feed_forward(x)\n",
        "\n",
        "        # Residual Connection and Layer Normalization\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# In the forward method, the input x undergoes multi-head self-attention. The resulting attention_output is added to the original x,\n",
        "# creating a residual connection. Layer normalization is then applied to the sum.\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04386d14",
      "metadata": {
        "id": "04386d14"
      },
      "source": [
        "he output of the layer normalization is passed through the position-wise feed-forward network. Again, a residual connection is established by adding the original x to the output of the feed-forward network. Layer normalization is applied again.\n",
        "\n",
        "The final output is returned, which represents the processed input after going through the multi-head self-attention, feed-forward network, and residual connections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5086443e",
      "metadata": {
        "id": "5086443e"
      },
      "source": [
        "## combine between decoder and encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee47c19",
      "metadata": {
        "id": "bee47c19"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, num_heads, d_ff, input_vocab_size, output_vocab_size, max_seq_length, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Input Embeddings\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layers = []\n",
        "        for _ in range(num_encoder_layers):\n",
        "            encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            encoder_layers.append(encoder_layer)\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        " #This line creates a sequential module (self.encoder) that consists of the encoder layers stored in encoder_layers.\n",
        "#The sequential module applies each encoder layer in sequence during the forward pass.\n",
        "\n",
        "        decoder_layers = []\n",
        "        for _ in range(num_decoder_layers):\n",
        "            decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            decoder_layers.append(decoder_layer)\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "    def forward(self, source, target, source_mask=None, target_mask=None):\n",
        "# This line defines the forward method for the Transformer model,\n",
        "#which specifies how input data is processed during the forward pass.\n",
        "\n",
        "# source: Input sequence (source) tensor.\n",
        "\n",
        "# target: Target sequence tensor (only used in sequence-to-sequence tasks).\n",
        "\n",
        "# source_mask=None: Mask for source sequence.\n",
        "\n",
        "# target_mask=None: Mask for target sequence.\n",
        "\n",
        "\n",
        "\n",
        "        # Input Embeddings\n",
        "        source = self.embedding(source)\n",
        "        source = self.positional_encoding(source)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_output = self.encoder(source, source_mask)\n",
        "\n",
        "        if target is not None:\n",
        "            target = self.embedding(target)\n",
        "            target = self.positional_encoding(target)\n",
        "            decoder_output = self.decoder(target, target_mask)\n",
        "        else:\n",
        "            decoder_output = None\n",
        "\n",
        "        if decoder_output is not None:\n",
        "            output = self.output_layer(decoder_output)\n",
        "        else:\n",
        "            output = None\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d92ea4",
      "metadata": {
        "id": "c9d92ea4"
      },
      "source": [
        "The forward method proceeds to process the input sequences through the model, which includes input embeddings, encoder layers, and the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2091978e",
      "metadata": {
        "id": "2091978e"
      },
      "source": [
        "## build text classification class to build the model and use it in train ,test,validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3247d359",
      "metadata": {
        "id": "3247d359"
      },
      "outputs": [],
      "source": [
        "class TransformerTextClassifier(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, d_model, num_heads, d_ff, input_vocab_size, num_classes, max_seq_length, dropout=0.1):\n",
        "        super(TransformerTextClassifier, self).__init__()\n",
        "# num_encoder_layers: The number of Transformer encoder layers in the model.\n",
        "# d_model: The dimensionality of the model's embeddings and hidden states.\n",
        "# num_heads: The number of attention heads in the multi-head self-attention mechanism.\n",
        "# d_ff: The size of the feedforward neural network within each encoder layer.\n",
        "# input_vocab_size: The size of the input vocabulary (number of unique tokens).\n",
        "# num_classes: The number of classes in the classification task.\n",
        "# max_seq_length: The maximum sequence length for input sequences.\n",
        "# dropout: A dropout rate applied throughout the model (default is 0.1).\n",
        "\n",
        "        # Input Embeddings\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Stack of Transformer Encoder Layers\n",
        "        encoder_layers = [] #A stack of Transformer encoder layers is created and stored in self.encoder\n",
        "        for _ in range(num_encoder_layers):\n",
        "            encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            encoder_layers.append(encoder_layer)\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Classification Head\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(d_model, 256),  # Example hidden layer size, adjust as needed\n",
        "#A linear layer (nn.Linear) that reduces the dimensionality of the transformer output to 256 (we can adjust this as needed).\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256, num_classes)  # Output layer with num_classes for classification #Another linear layer that produces the final classification scores with num_classes output units.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input Embeddings\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Classification Head\n",
        "        class_scores = self.classification_head(x[:, 0, :])  # Assuming you want to classify the whole sequence\n",
        "#The classification head operates on the output of the transformer, specifically on the hidden representation of the [CLS] token (the first token) by selecting x[:, 0, :].\n",
        "        return class_scores\n",
        "#The output of the classification head is a tensor of shape [batch_size, num_classes], containing the predicted class scores for each input sequence in the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903042c5",
      "metadata": {
        "id": "903042c5"
      },
      "source": [
        "The forward method proceeds to process the input sequence through the model, which includes input embeddings, multiple Transformer encoder layers, and the classification head."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79240c71",
      "metadata": {
        "id": "79240c71"
      },
      "source": [
        " It's designed for text classification, where you provide a sequence of tokens as input and get class scores as output, making it suitable for tasks like sentiment analysis, document classification, or any other text classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94cc333",
      "metadata": {
        "id": "d94cc333"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa013fe",
      "metadata": {
        "id": "1aa013fe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for read files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a31011",
      "metadata": {
        "id": "b7a31011"
      },
      "source": [
        "firstly ,we will read the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5413ddcd",
      "metadata": {
        "id": "5413ddcd"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"C:/Users/CLARA/Downloads/Attachment (4)/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9119a61d",
      "metadata": {
        "id": "9119a61d"
      },
      "source": [
        "we read the file and store it in df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a513472d",
      "metadata": {
        "id": "a513472d",
        "outputId": "b946f454-fecd-4ff5-b329-86d297c72862"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(159571, 8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape\n",
        "#to know the shape of data(rows/columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b663a48",
      "metadata": {
        "id": "7b663a48",
        "outputId": "dc33b783-766e-4ad7-8a2a-6b9febdfc085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   id             159571 non-null  object\n",
            " 1   comment_text   159571 non-null  object\n",
            " 2   toxic          159571 non-null  int64 \n",
            " 3   severe_toxic   159571 non-null  int64 \n",
            " 4   obscene        159571 non-null  int64 \n",
            " 5   threat         159571 non-null  int64 \n",
            " 6   insult         159571 non-null  int64 \n",
            " 7   identity_hate  159571 non-null  int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 9.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83133683",
      "metadata": {
        "id": "83133683"
      },
      "source": [
        "there isn't null values.comment_text,id is string but the the rest of data is  integer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f35104b",
      "metadata": {
        "id": "2f35104b"
      },
      "source": [
        "we will show first 10 rows to know about the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6e097f",
      "metadata": {
        "id": "2b6e097f",
        "outputId": "f92b5916-8c61-469e-e082-e88f78044994"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
              "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
              "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
              "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
              "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  \n",
              "5             0        0       0       0              0  \n",
              "6             1        1       0       1              0  \n",
              "7             0        0       0       0              0  \n",
              "8             0        0       0       0              0  \n",
              "9             0        0       0       0              0  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b21eb52",
      "metadata": {
        "id": "3b21eb52",
        "outputId": "27f0e3d0-a1a2-4e7e-d65c-82e7529258a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\"\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[3,'comment_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6564a9",
      "metadata": {
        "id": "cf6564a9"
      },
      "source": [
        "we see that we should clean the text data to begin the text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f651b9",
      "metadata": {
        "id": "24f651b9"
      },
      "source": [
        "## Clean the  text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3e2328",
      "metadata": {
        "id": "8a3e2328",
        "outputId": "5f5128b3-0a61-4111-8944-568a2998d587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: contractions in c:\\users\\clara\\appdata\\roaming\\python\\python39\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\clara\\appdata\\roaming\\python\\python39\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in c:\\users\\clara\\appdata\\roaming\\python\\python39\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: anyascii in c:\\users\\clara\\appdata\\roaming\\python\\python39\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15decb97",
      "metadata": {
        "id": "15decb97"
      },
      "outputs": [],
      "source": [
        "import string #to use it in removing punctuation\n",
        "import re #The re module stands for \"regular expressions,\" and it provides powerful tools for working with patterns and manipulating strings.\n",
        "#With the re module, we can perform various text operations, such as searching for patterns\n",
        "import contractions\n",
        "#This'contractions' library helps in expanding contractions in text, for example, converting \"I'm\" to \"I am\" or \"you'll\" to \"you will.\"\n",
        "#so when we remove punctuation donot affect the word so we should make it before removing Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ce4c18",
      "metadata": {
        "id": "40ce4c18"
      },
      "source": [
        "i will define function replace_not to use it in function clean(can\\not-->can not)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ede6ed8",
      "metadata": {
        "id": "3ede6ed8"
      },
      "outputs": [],
      "source": [
        "def replace_not(text):\n",
        "    # Define a regular expression pattern to match \"\\not\" with any number of backslashes\n",
        "    pattern = r'\\\\*not'\n",
        "\n",
        "    # Use re.sub() to replace the pattern with a space followed by \"not\"\n",
        "    transformed_text = re.sub(pattern, ' not', text)\n",
        "\n",
        "    return transformed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d98cf1",
      "metadata": {
        "id": "18d98cf1"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    # Convert text to lowercase\n",
        "    ans = text.lower()\n",
        "\n",
        "    # Expand contractions in text\n",
        "    ans = contractions.fix(ans)\n",
        "    ans=replace_not(ans)\n",
        "\n",
        "    # Remove repeating punctuation\n",
        "    ans = re.sub(r'(!|.)\\1+', '', ans)\n",
        "\n",
        "    PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "    # Remove punctuation\n",
        "    ans = ans.translate(str.maketrans('','', PUNCT_TO_REMOVE))\n",
        "\n",
        "    #replace middle dot\n",
        "    ans= ans.replace(\"·\", \"\")\n",
        "\n",
        "    # Remove digits\n",
        "    ans = ''.join([i for i in ans if not i.isdigit()])\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    ans = \" \".join(ans.split())\n",
        "\n",
        "    # Define a regular expression pattern for emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    # Remove emojis and symbols like ✆\n",
        "    ans = emoji_pattern.sub(r'', ans)\n",
        "    return ans\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0ca2d2",
      "metadata": {
        "id": "3b0ca2d2"
      },
      "outputs": [],
      "source": [
        "df['comment_text'] = df['comment_text'].apply(clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1cfdab",
      "metadata": {
        "id": "6e1cfdab"
      },
      "source": [
        "as we clean the  text data column(comment_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511097b1",
      "metadata": {
        "id": "511097b1",
        "outputId": "48c6696f-4361-41a9-a1a6-7506112d0976"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'more i can not make any real suestions on improvement i wondered if the section statistics should be later on or a subsection of types of aidentsi think the references may nd tidying so that they are a in the exact same format ie date format etc i can do that later on if noone else does first if you have any preferences for formaing style on references or want to do it yourself please let me know there aears to be a backlog on articles for review so i gue there may be a delay until a reviewer turns up it is listed in the relevant form eg wikipediagdarticlenominationstransport'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[3,'comment_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9229f474",
      "metadata": {
        "id": "9229f474"
      },
      "source": [
        "we will know if balanced or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9010f721",
      "metadata": {
        "id": "9010f721"
      },
      "outputs": [],
      "source": [
        "cols=['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
        "df['Toxic']=df[cols].max(axis=1).apply(lambda x: \"Toxic\" if x ==1 else \"Non-Toxic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac98297",
      "metadata": {
        "id": "bac98297"
      },
      "source": [
        "It uses the .max(axis=1) method to find the maximum value along each row of the specified columns (cols). Since these columns contain binary values (0 or 1), taking the maximum value effectively checks if any of the categories are marked as toxic for a given comment. If at least one category is marked as toxic (i.e., it has a value of 1), the maximum value for that row will be 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c375a1d",
      "metadata": {
        "id": "4c375a1d"
      },
      "source": [
        "the \"Toxic\" column is a binary label that indicates whether a comment is considered toxic or not based on the presence or absence of toxicity indicators in the specified columns. If any of the toxicity columns have a value of 1 for a particular comment, it is labeled as \"Toxic\"; otherwise, it is labeled as \"Non-Toxic.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73d8b3e",
      "metadata": {
        "id": "e73d8b3e"
      },
      "outputs": [],
      "source": [
        "clean = len(df[df.Toxic==\"Non-Toxic\"])\n",
        "toxic = len(df[df.Toxic==\"Toxic\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dfbead",
      "metadata": {
        "id": "07dfbead",
        "outputId": "16cf4fa3-818b-4f59-f088-1fa31c20b7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxic Comments count: 16225\n",
            "toxic percentage 10.17\n"
          ]
        }
      ],
      "source": [
        "print(\"Toxic Comments count:\",toxic)\n",
        "print(\"toxic percentage %.2f\"%(toxic/len(df)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d640e69",
      "metadata": {
        "id": "6d640e69",
        "outputId": "2742f923-bc32-40ed-88d5-55a639d1eb30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxic Comments count: 143346\n",
            "toxic percentage 89.83\n"
          ]
        }
      ],
      "source": [
        "print(\"Toxic Comments count:\",clean)\n",
        "print(\"toxic percentage %.2f\"%(clean/len(df)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d972652",
      "metadata": {
        "id": "7d972652"
      },
      "source": [
        "so we can see that data is not highly balanced so we will resovle it using regularization in transformer(dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3cf845",
      "metadata": {
        "id": "bf3cf845"
      },
      "source": [
        "convert toxic to 1 and non toxic to zaro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888dd816",
      "metadata": {
        "id": "888dd816"
      },
      "outputs": [],
      "source": [
        "df['Toxic']=df['Toxic'].apply(lambda x:1 if x =='Toxic' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d836637",
      "metadata": {
        "id": "8d836637"
      },
      "source": [
        "### Apply tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1053c9fd",
      "metadata": {
        "id": "1053c9fd",
        "outputId": "7104bb5b-f07b-4281-bae8-07924b8de5e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\CLARA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk #for tokenization\n",
        "# Ensure the 'punkt' resource is downloaded\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccba9023",
      "metadata": {
        "id": "ccba9023"
      },
      "outputs": [],
      "source": [
        "# the tokenization function\n",
        "def token(text):\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95927ebc",
      "metadata": {
        "id": "95927ebc"
      },
      "source": [
        "Tokenize the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1896f87",
      "metadata": {
        "id": "d1896f87"
      },
      "outputs": [],
      "source": [
        "df['comment_text2'] =df['comment_text'].apply(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245a1c09",
      "metadata": {
        "id": "245a1c09",
        "outputId": "f77281ba-6568-45c1-b8f9-4b0290e686be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['more',\n",
              " 'i',\n",
              " 'can',\n",
              " 'not',\n",
              " 'make',\n",
              " 'any',\n",
              " 'real',\n",
              " 'suestions',\n",
              " 'on',\n",
              " 'improvement',\n",
              " 'i',\n",
              " 'wondered',\n",
              " 'if',\n",
              " 'the',\n",
              " 'section',\n",
              " 'statistics',\n",
              " 'should',\n",
              " 'be',\n",
              " 'later',\n",
              " 'on',\n",
              " 'or',\n",
              " 'a',\n",
              " 'subsection',\n",
              " 'of',\n",
              " 'types',\n",
              " 'of',\n",
              " 'aidentsi',\n",
              " 'think',\n",
              " 'the',\n",
              " 'references',\n",
              " 'may',\n",
              " 'nd',\n",
              " 'tidying',\n",
              " 'so',\n",
              " 'that',\n",
              " 'they',\n",
              " 'are',\n",
              " 'a',\n",
              " 'in',\n",
              " 'the',\n",
              " 'exact',\n",
              " 'same',\n",
              " 'format',\n",
              " 'ie',\n",
              " 'date',\n",
              " 'format',\n",
              " 'etc',\n",
              " 'i',\n",
              " 'can',\n",
              " 'do',\n",
              " 'that',\n",
              " 'later',\n",
              " 'on',\n",
              " 'if',\n",
              " 'noone',\n",
              " 'else',\n",
              " 'does',\n",
              " 'first',\n",
              " 'if',\n",
              " 'you',\n",
              " 'have',\n",
              " 'any',\n",
              " 'preferences',\n",
              " 'for',\n",
              " 'formaing',\n",
              " 'style',\n",
              " 'on',\n",
              " 'references',\n",
              " 'or',\n",
              " 'want',\n",
              " 'to',\n",
              " 'do',\n",
              " 'it',\n",
              " 'yourself',\n",
              " 'please',\n",
              " 'let',\n",
              " 'me',\n",
              " 'know',\n",
              " 'there',\n",
              " 'aears',\n",
              " 'to',\n",
              " 'be',\n",
              " 'a',\n",
              " 'backlog',\n",
              " 'on',\n",
              " 'articles',\n",
              " 'for',\n",
              " 'review',\n",
              " 'so',\n",
              " 'i',\n",
              " 'gue',\n",
              " 'there',\n",
              " 'may',\n",
              " 'be',\n",
              " 'a',\n",
              " 'delay',\n",
              " 'until',\n",
              " 'a',\n",
              " 'reviewer',\n",
              " 'turns',\n",
              " 'up',\n",
              " 'it',\n",
              " 'is',\n",
              " 'listed',\n",
              " 'in',\n",
              " 'the',\n",
              " 'relevant',\n",
              " 'form',\n",
              " 'eg',\n",
              " 'wikipediagdarticlenominationstransport']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[3,'comment_text2']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72adffe5",
      "metadata": {
        "id": "72adffe5"
      },
      "source": [
        "so we cannot remove stop words as that it  affects the meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33125303",
      "metadata": {
        "id": "33125303"
      },
      "source": [
        "## Apply stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1284b52",
      "metadata": {
        "id": "b1284b52"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stem(text):\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in text]#to make stemming as to restore rhe words to the origin words\n",
        "    #it takes every token and restore into origin word\n",
        "    return stemmed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9a16f2",
      "metadata": {
        "id": "bb9a16f2"
      },
      "source": [
        "Apply stemming on text (tokens) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd39bba",
      "metadata": {
        "id": "abd39bba"
      },
      "outputs": [],
      "source": [
        "df['comment_text2'] =df['comment_text2'].apply(stem)#transform words into a common form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da97679",
      "metadata": {
        "id": "0da97679",
        "outputId": "8e3b3ca8-fbe7-4069-e7df-42e980d59835"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['more',\n",
              " 'i',\n",
              " 'can',\n",
              " 'not',\n",
              " 'make',\n",
              " 'ani',\n",
              " 'real',\n",
              " 'suestion',\n",
              " 'on',\n",
              " 'improv',\n",
              " 'i',\n",
              " 'wonder',\n",
              " 'if',\n",
              " 'the',\n",
              " 'section',\n",
              " 'statist',\n",
              " 'should',\n",
              " 'be',\n",
              " 'later',\n",
              " 'on',\n",
              " 'or',\n",
              " 'a',\n",
              " 'subsect',\n",
              " 'of',\n",
              " 'type',\n",
              " 'of',\n",
              " 'aidentsi',\n",
              " 'think',\n",
              " 'the',\n",
              " 'refer',\n",
              " 'may',\n",
              " 'nd',\n",
              " 'tidi',\n",
              " 'so',\n",
              " 'that',\n",
              " 'they',\n",
              " 'are',\n",
              " 'a',\n",
              " 'in',\n",
              " 'the',\n",
              " 'exact',\n",
              " 'same',\n",
              " 'format',\n",
              " 'ie',\n",
              " 'date',\n",
              " 'format',\n",
              " 'etc',\n",
              " 'i',\n",
              " 'can',\n",
              " 'do',\n",
              " 'that',\n",
              " 'later',\n",
              " 'on',\n",
              " 'if',\n",
              " 'noon',\n",
              " 'els',\n",
              " 'doe',\n",
              " 'first',\n",
              " 'if',\n",
              " 'you',\n",
              " 'have',\n",
              " 'ani',\n",
              " 'prefer',\n",
              " 'for',\n",
              " 'forma',\n",
              " 'style',\n",
              " 'on',\n",
              " 'refer',\n",
              " 'or',\n",
              " 'want',\n",
              " 'to',\n",
              " 'do',\n",
              " 'it',\n",
              " 'yourself',\n",
              " 'pleas',\n",
              " 'let',\n",
              " 'me',\n",
              " 'know',\n",
              " 'there',\n",
              " 'aear',\n",
              " 'to',\n",
              " 'be',\n",
              " 'a',\n",
              " 'backlog',\n",
              " 'on',\n",
              " 'articl',\n",
              " 'for',\n",
              " 'review',\n",
              " 'so',\n",
              " 'i',\n",
              " 'gue',\n",
              " 'there',\n",
              " 'may',\n",
              " 'be',\n",
              " 'a',\n",
              " 'delay',\n",
              " 'until',\n",
              " 'a',\n",
              " 'review',\n",
              " 'turn',\n",
              " 'up',\n",
              " 'it',\n",
              " 'is',\n",
              " 'list',\n",
              " 'in',\n",
              " 'the',\n",
              " 'relev',\n",
              " 'form',\n",
              " 'eg',\n",
              " 'wikipediagdarticlenominationstransport']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[3,'comment_text2']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72172833",
      "metadata": {
        "id": "72172833"
      },
      "source": [
        "i will choose 40% as first 63828 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a27d23",
      "metadata": {
        "id": "d4a27d23"
      },
      "source": [
        "we should know the number of  stemming tokens in 63828 40% before using in transformer as input vocab size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5509a240",
      "metadata": {
        "id": "5509a240"
      },
      "outputs": [],
      "source": [
        "unique_tokens = set(word for sentence_tokens in df['comment_text2'][:63828] for word in sentence_tokens)\n",
        "input_vocab_size = len(unique_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d655ba0",
      "metadata": {
        "id": "3d655ba0"
      },
      "source": [
        "we will know vocab input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c09490",
      "metadata": {
        "id": "25c09490",
        "outputId": "4ea6ed52-600c-4109-c3a3-d3c0b5798700"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128728"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e304e50",
      "metadata": {
        "id": "9e304e50"
      },
      "source": [
        "128728 input voab size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857df42a",
      "metadata": {
        "id": "857df42a"
      },
      "source": [
        "we should know max_length of input to use it in transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e638698",
      "metadata": {
        "id": "8e638698",
        "outputId": "8abda94a-6e6a-47b2-efee-20cf3a7fb9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length in the column of text: 1403\n"
          ]
        }
      ],
      "source": [
        "max_length = 0\n",
        "\n",
        "# Iterate through tokenized sequences and calculate lengths\n",
        "for sentence_tokens in df['comment_text2'][:63828]:\n",
        "    sequence_length = len(sentence_tokens)\n",
        "    if sequence_length > max_length:\n",
        "        max_length = sequence_length\n",
        "\n",
        "print(\"Maximum sequence length in the column of text:\", max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303dde4b",
      "metadata": {
        "id": "303dde4b"
      },
      "source": [
        "max_length of input 1403"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bba775e",
      "metadata": {
        "id": "0bba775e"
      },
      "source": [
        "## Apply Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd886be2",
      "metadata": {
        "id": "fd886be2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "#The Counter class is a convenient and useful tool for counting the occurrences of elements in a collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5559949",
      "metadata": {
        "id": "a5559949"
      },
      "source": [
        "Function about bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a4c653",
      "metadata": {
        "id": "61a4c653"
      },
      "outputs": [],
      "source": [
        "def bagOfWords(text):#Python function called bagOfWords(text) that takes a list of words as input and returns a Bag of Words (BoW) vector representing the word frequencies in that list\n",
        "# Create a vocabulary from all the unique words\n",
        "    vocab = sorted(set(text))\n",
        "    bow_vector = np.zeros(len(vocab))# The length of this array is equal to the size of the vocabulary created in the previous step\n",
        "\n",
        "# Count word occurrences and fill in the BoW vector\n",
        "    word_counts = Counter(text)\n",
        "# This line uses the Counter function from Python's collections module to count the occurrences of each word in the input list (text). It creates a dictionary-like object (word_counts) where each word is a key, and its count is the associated value.\n",
        "    for word, count in word_counts.items():\n",
        "        word_index = vocab.index(word)#This line finds the index of the current word in the sorted vocabulary (vocab).\n",
        "        bow_vector[word_index] = count#how many times it appears in the input list\n",
        "    return bow_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedd1a25",
      "metadata": {
        "id": "cedd1a25"
      },
      "source": [
        "Apply bag of words to know the occurrences of each stemming token and store it in new column in df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f464a4f8",
      "metadata": {
        "id": "f464a4f8"
      },
      "outputs": [],
      "source": [
        "df['comment_Bag'] =df['comment_text2'].apply(bagOfWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9ac572",
      "metadata": {
        "id": "9b9ac572",
        "outputId": "1e1e9a5a-475c-4382-886f-2b21d35b712b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5., 1., 1., 2., 1., 1., 1., 3., 2., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
              "       1., 2., 1., 1., 2., 1., 1., 4., 1., 3., 1., 2., 1., 2., 1., 2., 1.,\n",
              "       1., 1., 2., 1., 1., 1., 1., 1., 2., 5., 2., 1., 1., 1., 2., 1., 2.,\n",
              "       1., 1., 1., 2., 1., 1., 1., 1., 2., 4., 2., 1., 1., 1., 2., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[3,'comment_Bag']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6e3b58",
      "metadata": {
        "id": "fb6e3b58"
      },
      "source": [
        "import libraries to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b20c9d08",
      "metadata": {
        "id": "b20c9d08"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split # to split the data\n",
        "from torch.utils.data import Dataset, DataLoader.\n",
        "#class is another essential component in PyTorch for handling data.\n",
        "#It helps in creating iterable data loaders that can be used in training and testing loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f76db2e4",
      "metadata": {
        "id": "f76db2e4"
      },
      "source": [
        "convert every row to tenser to use in transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc5a983",
      "metadata": {
        "id": "fdc5a983"
      },
      "outputs": [],
      "source": [
        "def make_tensor(row):\n",
        "    return torch.tensor(row, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea9f7a0",
      "metadata": {
        "id": "bea9f7a0"
      },
      "source": [
        "convert to tensor and store it in new column in df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37eb4a2f",
      "metadata": {
        "id": "37eb4a2f"
      },
      "outputs": [],
      "source": [
        "df['comment_text3'] = df['comment_Bag'].apply(make_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6982a705",
      "metadata": {
        "id": "6982a705"
      },
      "outputs": [],
      "source": [
        "X=df['comment_text3'][:63828] #40%\n",
        "y=df['Toxic'][:63828]#40%\n",
        "y=torch.tensor(y)#convert target row to tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0d15f4",
      "metadata": {
        "id": "ef0d15f4",
        "outputId": "e9421a03-3c5c-4e88-f266-fd637704309a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 0, 0, 0])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4508161c",
      "metadata": {
        "id": "4508161c"
      },
      "source": [
        "Split X and y into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d50c99",
      "metadata": {
        "id": "a3d50c99"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1833a1",
      "metadata": {
        "id": "cc1833a1"
      },
      "source": [
        "### Apply padding on x train tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3bc336",
      "metadata": {
        "id": "8f3bc336"
      },
      "source": [
        "we should apply padding before use it in data loader before used it in model train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe73b01",
      "metadata": {
        "id": "4fe73b01",
        "outputId": "ab7d781e-9f9c-4c7b-f156-4b5386bc08b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[5, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [3, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = max(len(tensor) for tensor in X_train)# tow know max length tensor in X_train\n",
        "X_train_padded = [torch.cat([tensor, torch.zeros(max_length - len(tensor))]) for tensor in X_train]#padding with zeros tensors\n",
        "\n",
        "X_train_stacked = torch.stack(X_train_padded, dim=0)#This line stacks the tensors in the list X_train_padded along dimension 0.\n",
        "X_train_stacked=X_train_stacked.long()#convert it to long\n",
        "X_train_stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe2f5a6",
      "metadata": {
        "id": "cfe2f5a6",
        "outputId": "fd1592a8-6e08-48a4-c922-65e1b273236f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x205fea89ac0>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 32# Adjust as needed\n",
        "train_dataset = list(zip(X_train_stacked, y_train))#combine between Xtrain,ytrain\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)#make dataloader for train dataset\n",
        "#The DataLoader class is another essential component in PyTorch for handling data.\n",
        "#It helps in creating iterable data loaders that can be used in training\n",
        "# Now, train_loader contains both X_train and y_train in batches for training.\n",
        "train_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fac4b4",
      "metadata": {
        "id": "f2fac4b4"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "num_encoder_layers = 6\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "input_vocab_size = 128728  # Replace with your actual vocabulary size\n",
        "num_classes = 2  # Number of classes for classification\n",
        "max_seq_length =1403 # Maximum sequence length in your data\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7524b567",
      "metadata": {
        "id": "7524b567"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim# to use to optimize the model i will use it during training\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12534d50",
      "metadata": {
        "id": "12534d50",
        "outputId": "44a2e792-d3bf-4901-c0a7-3ef7045cdba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 0.3364\n"
          ]
        }
      ],
      "source": [
        "# Create the Text Classification Transformer model\n",
        "text_classifier = TransformerTextClassifier(num_encoder_layers, d_model, num_heads, d_ff, input_vocab_size, num_classes, max_seq_length, dropout)\n",
        "# make object text_classifier of class TransformerTextClassifier\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(text_classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop# Training loop\n",
        "epochs = 1 #This sets the number of training epochs.\n",
        "for epoch in range(epochs):#This initiates a loop that iterates for the specified number of epochs\n",
        "    text_classifier.train()\n",
        "    #This sets the text_classifier model in training mode.\n",
        "    total_loss = 0.0 #This initializes a variable to keep track of the total loss for the current epoch\n",
        "    for batch_x, batch_y in train_loader:\n",
        "#train_loader. In each iteration, batch_x represents a batch of input data, and batch_y represents the corresponding batch of target labels.\n",
        "        optimizer.zero_grad()\n",
        "#It's necessary to reset gradients at the beginning of each batch before computing gradients during backpropagation\n",
        "        outputs = text_classifier(batch_x)#The model should output class scores for each input sample.\n",
        "        loss = criterion(outputs, batch_y)\n",
        "#. The criterion is typically a loss function like cross-entropy loss for classification tasks.\n",
        "        loss.backward()#This computes gradients for each model parameter with respect to the loss.\n",
        "        optimizer.step()#This updates the model's parameters using the optimizer\n",
        "        total_loss += loss.item()#This accumulates the current batch's loss into total_loss.\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "#After processing all batches in the epoch, this computes the average loss for the epoch by dividing total_loss by the number of batches (len(train_loader)).\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f45fe41",
      "metadata": {
        "id": "9f45fe41"
      },
      "source": [
        "### Apply padding to X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d1da24",
      "metadata": {
        "id": "e7d1da24"
      },
      "source": [
        "we should apply padding before use it in data loader before used it in model evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f32b48",
      "metadata": {
        "id": "b7f32b48",
        "outputId": "2addaafc-f565-4db9-e4ae-0bc772727e41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [6, 1, 3,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [4, 1, 1,  ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = max(len(tensor) for tensor in X_test)# tow know max length tensor in X_test\n",
        "X_test_padded = [torch.cat([tensor, torch.zeros(max_length - len(tensor))]) for tensor in X_test]##padding with zeros tensors\n",
        "X_test_stacked = torch.stack(X_test_padded, dim=0)##This line stacks the tensors in the list X_test_padded along dimension 0.\n",
        "X_test_stacked=X_test_stacked.long()##convert it to long\n",
        "X_test_stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61003f71",
      "metadata": {
        "id": "61003f71"
      },
      "outputs": [],
      "source": [
        "batch_size = 32# Adjust as needed\n",
        "test_dataset = list(zip(X_test_stacked, y_test))##combine between Xtest,ytest\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Now, test_loader contains both X_test and y_test in batches for test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a2d5fd",
      "metadata": {
        "id": "a4a2d5fd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac05ce6a",
      "metadata": {
        "id": "ac05ce6a",
        "outputId": "52f98894-d1a3-4717-c8b1-00284e330c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8955\n"
          ]
        }
      ],
      "source": [
        "text_classifier.eval()  # Set the model to evaluation mode\n",
        "# Lists to store predictions and ground truth labels\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():#is used to disable gradient computation during evaluation\n",
        "    for test_batch_x, test_batch_y in test_loader:\n",
        "#test_batch_x represents a batch of input data, and test_batch_y represents the corresponding batch of target labels (ground truth).\n",
        "        test_outputs = text_classifier(test_batch_x)# The model should output class scores for each input sample.\n",
        "        test_predictions.extend(torch.argmax(test_outputs, dim=1).tolist())\n",
        "#It uses torch.argmax(test_outputs, dim=1) to find the index of the class with the highest score for each sample and then converts the result to a Python list\n",
        "        test_targets.extend(test_batch_y.tolist())\n",
        "#This line appends the actual ground truth labels (test_batch_y) for the current batch to the test_targets list.\n",
        "# Calculate accuracy using the accuracy_score function\n",
        "accuracy = accuracy_score(test_targets, test_predictions)#the code prints the test accuracy, which indicates how well the model performs on the test dataset.\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda28a1e",
      "metadata": {
        "id": "bda28a1e"
      },
      "source": [
        "so it works well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47db0a0",
      "metadata": {
        "id": "c47db0a0"
      },
      "source": [
        "### In the end, I thank you for your sprints team effort"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}